---
title: "The incremental log-sum-exp trick"
mathjax: true
categories:
  - Machine Learning
---

Often in machine learning it is necessary to compute the following quantity. We have a $$n$$-dimensional vector $$\mathbf x$$---usually log-probabilities--- and we want to calculate:

$$y = \log \sum_{i=1}^n \exp^{x_i}.$$

Calculating the quantity in a naive manner we can encounter underflows or overflows. For an arbitrary $$a$$, we can shift the center of the exponential sum in the following way:

$$\log \sum_{i=1}^n \exp^{x_i} = a + \log \sum_{i=1}^n \exp^{x_i-a}.$$

A typical value for $$a$$ is setting it to the maximum, $$a = \max_{1\leq i \leq n} x_i$$, thus forcing the greatest value to be zero.

However, if the values are available in an incremental way we cannot compute the maximum value. The alternative incremental log-sum-exp trick can be computed using the following recurrence:

$$\max_{0} = x_0$$

if $$x_t > \max_{t-1}$$ then 

$$y_t = y_{t-1} \exp^{\max_{t-1} - x_t} + y_{t-1} +  1 \\ \max_{t} = x_t$$

otherwise,

$$y_t = y_{t-1} +  \exp^{x_t - \max_{t-1}} \\ \max_{t} = \max_{t-1}.$$
